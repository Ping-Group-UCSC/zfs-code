#!/bin/bash
#SBATCH --job-name=pwx
#SBATCH --output=qe.%j
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=16
#SBATCH --time=24:00:00
#SBATCH --partition=normal
##SBATCH --dependency=afterany:48083
#SBATCH --account=normal
##SBATCH --account=cfn307395

#################### Kairay ################################################################
 module add intel/17.0.5.239 impi/2017
 export OMP_NUM_THREADS=1
 NCORES=$(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES))
# MPICMD="mpirun -genv I_MPI_FABRICS shm:ofa -n $SLURM_NTASKS"
# MPICMD="mpiexec.hydra -genv I_MPI_FABRICS shm:ofa -n $SLURM_NTASKS"
MPICMD="mpiexec.hydra -genv I_MPI_FABRICS shm:ofa -n $NCORES"
 PWDIR="/export/data/share/wufeng/programs-intel2017.5/qe-6.1-scal/bin"
# PWDIR="/home/kli103/work/programs/qe-6.4.1_hdf5/bin"
# ENV='/home/szhan213/miniconda3/envs/pyzfs/bin'
# VASPDIR="/export/data/share/wufeng/programs-intel2017.5/vasp/vasp.5.4.4-vtst/bin/vasp_std"

# module load gnu openmpi mkl gsl
# MPICMD="mpirun --mca btl openib,sm,self --bind-to none -n $SLURM_NTASKS"
# JDFTXDIR=/export/data/share/wufeng/share/programs/JDFTX-fix20180209/build/
############################################################################################


######################### Stampede2 #####################################
# MPICMD="ibrun"
# PWDIR=/home1/06931/kli1003/work/programs/qe-6.1.0/bin
#########################################################################


#################################### Lux ############################################################################################################
# hostname
# echo "Running program on $SLURM_JOB_NUM_NODES nodes with $SLURM_NTASKS total tasks, with each node getting $SLURM_NTASKS_PER_NODE running on cores."
# module load intel/impi
# MPICMD="mpirun -n $SLURM_NTASKS --ppn 40"
# PWDIR=/data/users/jxu153/codes/qe/qe-6.1.0/bin
# PWDIR=/data/users/jxu153/codes/qe/qe-6.4.1/bin
# PWDIR=/data/groups/ping/kli103/programs/qe-6.6/bin
#####################################################################################################################################################


####################################### Bridges2 ####################################################################################################
# echo "Running program on $SLURM_JOB_NUM_NODES nodes with $SLURM_NTASKS total tasks, with each node getting $SLURM_NTASKS_PER_NODE running on cores."
# module load intelmpi
# export OMP_NUM_THREADS=1
# MPICMD="mpirun -n $SLURM_NTASKS --ppn 40"
# PWDIR=/jet/home/kli103/programs/qe-6.1.0/bin
# PWDIR=/jet/home/kli103/programs/qe-6.6/bin
# PWDIR=/jet/home/kli103/programs/qe-6.7.0/bin
#####################################################################################################################################################


############################### BNL ###########################
# module load intel
# echo "Start:"; date
# export OMP_NUM_THREADS=1
# MPICMD="srun -n $SLURM_NTASKS"
# MPICMDS="mpirun -n 1"
# PWDIR="/sdcc/u/kli/programs/qe-6.1.0/bin"
# PWDIR="/sdcc/u/kli/programs/qe-6.6/bin"
###############################################################

#Require: .save file and scf.out in same directory
echo "Start:"; date

 $MPICMD $PWDIR/pw.x -nk 2 -nb 2 -nd 1 -inp scf.in > scf.out

